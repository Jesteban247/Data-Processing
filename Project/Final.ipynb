{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease [3631 B]\n",
      "Hit:3 https://dl.yarnpkg.com/debian stable InRelease                           \u001b[0m\n",
      "Get:4 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease [3960 B]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease   \u001b[0m\u001b[33m\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:7 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main amd64 Packages [258 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1174 kB]\u001b[33m\u001b[33m\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3330 kB]33m\u001b[33m\n",
      "Hit:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease              \u001b[0m\n",
      "Get:12 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable/main amd64 Packages [3828 B]m\n",
      "Hit:9 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease            \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1470 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3807 kB]\n",
      "Fetched 10.3 MB in 2s (5816 kB/s)   \u001b[0m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyspark in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: py4j in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/05 19:18:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d23d2344-7fe0-40d8-b569-1577eefee4d4.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f03e73132e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Example\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrest_key</th>\n",
       "      <th>arrest_date</th>\n",
       "      <th>pd_cd</th>\n",
       "      <th>pd_desc</th>\n",
       "      <th>ky_cd</th>\n",
       "      <th>ofns_desc</th>\n",
       "      <th>law_code</th>\n",
       "      <th>law_cat_cd</th>\n",
       "      <th>arrest_boro</th>\n",
       "      <th>arrest_precinct</th>\n",
       "      <th>...</th>\n",
       "      <th>x_coord_cd</th>\n",
       "      <th>y_coord_cd</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geocoded_column</th>\n",
       "      <th>:@computed_region_f5dn_yrer</th>\n",
       "      <th>:@computed_region_yeji_bk3q</th>\n",
       "      <th>:@computed_region_92fq_4b7q</th>\n",
       "      <th>:@computed_region_sbqj_enih</th>\n",
       "      <th>:@computed_region_efsh_h5xi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261265483</td>\n",
       "      <td>2023-01-03T00:00:00.000</td>\n",
       "      <td>397</td>\n",
       "      <td>ROBBERY,OPEN AREA UNCLASSIFIED</td>\n",
       "      <td>105</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>PL 1600500</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>1027430</td>\n",
       "      <td>251104</td>\n",
       "      <td>40.855793</td>\n",
       "      <td>-73.843908</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-73.843908, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>11270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>261271301</td>\n",
       "      <td>2023-01-03T00:00:00.000</td>\n",
       "      <td>105</td>\n",
       "      <td>STRANGULATION 1ST</td>\n",
       "      <td>106</td>\n",
       "      <td>FELONY ASSAULT</td>\n",
       "      <td>PL 1211200</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>962808</td>\n",
       "      <td>174275</td>\n",
       "      <td>40.644996</td>\n",
       "      <td>-74.077263</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-74.077263, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>74</td>\n",
       "      <td>10369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261336449</td>\n",
       "      <td>2023-01-04T00:00:00.000</td>\n",
       "      <td>397</td>\n",
       "      <td>ROBBERY,OPEN AREA UNCLASSIFIED</td>\n",
       "      <td>105</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>PL 1601001</td>\n",
       "      <td>F</td>\n",
       "      <td>K</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>995118</td>\n",
       "      <td>155708</td>\n",
       "      <td>40.594054</td>\n",
       "      <td>-73.960866</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-73.960866, ...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>18183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>261328047</td>\n",
       "      <td>2023-01-04T00:00:00.000</td>\n",
       "      <td>105</td>\n",
       "      <td>STRANGULATION 1ST</td>\n",
       "      <td>106</td>\n",
       "      <td>FELONY ASSAULT</td>\n",
       "      <td>PL 1211200</td>\n",
       "      <td>F</td>\n",
       "      <td>Q</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>1007694</td>\n",
       "      <td>219656</td>\n",
       "      <td>40.769552</td>\n",
       "      <td>-73.915361</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-73.915361, ...</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>16860.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261417496</td>\n",
       "      <td>2023-01-05T00:00:00.000</td>\n",
       "      <td>244</td>\n",
       "      <td>BURGLARY,UNCLASSIFIED,UNKNOWN</td>\n",
       "      <td>107</td>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>PL 1402000</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>1007174</td>\n",
       "      <td>239542</td>\n",
       "      <td>40.824135</td>\n",
       "      <td>-73.917170</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-73.91717, 4...</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>10929.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   arrest_key              arrest_date  pd_cd                         pd_desc  \\\n",
       "0   261265483  2023-01-03T00:00:00.000    397  ROBBERY,OPEN AREA UNCLASSIFIED   \n",
       "1   261271301  2023-01-03T00:00:00.000    105               STRANGULATION 1ST   \n",
       "2   261336449  2023-01-04T00:00:00.000    397  ROBBERY,OPEN AREA UNCLASSIFIED   \n",
       "3   261328047  2023-01-04T00:00:00.000    105               STRANGULATION 1ST   \n",
       "4   261417496  2023-01-05T00:00:00.000    244   BURGLARY,UNCLASSIFIED,UNKNOWN   \n",
       "\n",
       "   ky_cd       ofns_desc    law_code law_cat_cd arrest_boro  arrest_precinct  \\\n",
       "0    105         ROBBERY  PL 1600500          F           B               49   \n",
       "1    106  FELONY ASSAULT  PL 1211200          F           S              120   \n",
       "2    105         ROBBERY  PL 1601001          F           K               61   \n",
       "3    106  FELONY ASSAULT  PL 1211200          F           Q              114   \n",
       "4    107        BURGLARY  PL 1402000          F           B               44   \n",
       "\n",
       "   ...  x_coord_cd y_coord_cd   latitude  longitude  \\\n",
       "0  ...     1027430     251104  40.855793 -73.843908   \n",
       "1  ...      962808     174275  40.644996 -74.077263   \n",
       "2  ...      995118     155708  40.594054 -73.960866   \n",
       "3  ...     1007694     219656  40.769552 -73.915361   \n",
       "4  ...     1007174     239542  40.824135 -73.917170   \n",
       "\n",
       "                                     geocoded_column  \\\n",
       "0  {'type': 'Point', 'coordinates': [-73.843908, ...   \n",
       "1  {'type': 'Point', 'coordinates': [-74.077263, ...   \n",
       "2  {'type': 'Point', 'coordinates': [-73.960866, ...   \n",
       "3  {'type': 'Point', 'coordinates': [-73.915361, ...   \n",
       "4  {'type': 'Point', 'coordinates': [-73.91717, 4...   \n",
       "\n",
       "   :@computed_region_f5dn_yrer  :@computed_region_yeji_bk3q  \\\n",
       "0                           59                            5   \n",
       "1                            4                            1   \n",
       "2                           32                            2   \n",
       "3                           39                            3   \n",
       "4                           34                            5   \n",
       "\n",
       "   :@computed_region_92fq_4b7q :@computed_region_sbqj_enih  \\\n",
       "0                           12                          32   \n",
       "1                           13                          74   \n",
       "2                           15                          36   \n",
       "3                            4                          72   \n",
       "4                           43                          25   \n",
       "\n",
       "   :@computed_region_efsh_h5xi  \n",
       "0                      11270.0  \n",
       "1                      10369.0  \n",
       "2                      18183.0  \n",
       "3                      16860.0  \n",
       "4                      10929.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"https://data.cityofnewyork.us/resource/uip8-fykc.json\")\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(local_file_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show the first 5 rows\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download the JSON file locally\n",
    "json_url = \"https://data.cityofnewyork.us/resource/uip8-fykc.json\"\n",
    "local_file_path = \"/workspaces/Procesamiento-Datos/Project/Data/data.json\"\n",
    "urllib.request.urlretrieve(json_url, local_file_path)\n",
    "\n",
    "# Read local JSON file into Spark DataFrame\n",
    "spark_df = spark.read.json(local_file_path)\n",
    "\n",
    "# Show the first 5 rows\n",
    "spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows without considering corrupt records\n",
    "spark_df.drop(\"_corrupt_record\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+-----+--------------+----------+----------+-----------+---------------+-----------------+---------+--------+---------+----------+----------+---------+----------+--------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\n",
      "|arrest_key|         arrest_date|pd_cd|             pd_desc|ky_cd|     ofns_desc|  law_code|law_cat_cd|arrest_boro|arrest_precinct|jurisdiction_code|age_group|perp_sex|perp_race|x_coord_cd|y_coord_cd| latitude| longitude|     geocoded_column|:@computed_region_f5dn_yrer|:@computed_region_yeji_bk3q|:@computed_region_92fq_4b7q|:@computed_region_sbqj_enih|:@computed_region_efsh_h5xi|\n",
      "+----------+--------------------+-----+--------------------+-----+--------------+----------+----------+-----------+---------------+-----------------+---------+--------+---------+----------+----------+---------+----------+--------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\n",
      "| 261265483|2023-01-03T00:00:...|  397|ROBBERY,OPEN AREA...|  105|       ROBBERY|PL 1600500|         F|          B|             49|                0|    18-24|       M|    BLACK|   1027430|    251104|40.855793|-73.843908|{coordinates -> [...|                         59|                          5|                         12|                         32|                    11270.0|\n",
      "| 261271301|2023-01-03T00:00:...|  105|   STRANGULATION 1ST|  106|FELONY ASSAULT|PL 1211200|         F|          S|            120|                0|    25-44|       M|    WHITE|    962808|    174275|40.644996|-74.077263|{coordinates -> [...|                          4|                          1|                         13|                         74|                    10369.0|\n",
      "| 261336449|2023-01-04T00:00:...|  397|ROBBERY,OPEN AREA...|  105|       ROBBERY|PL 1601001|         F|          K|             61|                0|      <18|       M|    BLACK|    995118|    155708|40.594054|-73.960866|{coordinates -> [...|                         32|                          2|                         15|                         36|                    18183.0|\n",
      "| 261328047|2023-01-04T00:00:...|  105|   STRANGULATION 1ST|  106|FELONY ASSAULT|PL 1211200|         F|          Q|            114|                0|    18-24|       M|    BLACK|   1007694|    219656|40.769552|-73.915361|{coordinates -> [...|                         39|                          3|                          4|                         72|                    16860.0|\n",
      "| 261417496|2023-01-05T00:00:...|  244|BURGLARY,UNCLASSI...|  107|      BURGLARY|PL 1402000|         F|          B|             44|                0|    25-44|       F|    BLACK|   1007174|    239542|40.824135| -73.91717|{coordinates -> [...|                         34|                          5|                         43|                         25|                    10929.0|\n",
      "+----------+--------------------+-----+--------------------+-----+--------------+----------+----------+-----------+---------------+-----------------+---------+--------+---------+----------+----------+---------+----------+--------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borough                                     region   Males Females  \\\n",
      "0   Bronx         Riverdale, Fieldston & Kingsbridge  52,133  61,937   \n",
      "1               Wakefield, Williamsbridge & Woodlawn  65,087  77,848   \n",
      "2             Co-op City, Pelham Bay & Schuylerville  55,615  65,929   \n",
      "3              Pelham Parkway, Morris Park & Laconia  61,233  67,896   \n",
      "4          Belmont, Crotona Park East & East Tremont  75,963  87,740   \n",
      "\n",
      "  Total Population  \n",
      "0          114,070  \n",
      "1          142,935  \n",
      "2          121,544  \n",
      "3          129,130  \n",
      "4          163,704  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoodpop.htm\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table with the specified class\n",
    "table = soup.find('table', {'class': 'light_table right'})\n",
    "\n",
    "# Extract the table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extracting data from each row\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(['th', 'td'])\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "columns = data[0]  # Assuming the first row contains column headers\n",
    "df = pd.DataFrame(data[1:], columns=columns)\n",
    "\n",
    "# Display the head of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 401\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual OpenWeatherMap API key\n",
    "api_key = '69f9153f8e4f7e6c52715b0b75ababe3'\n",
    "latitude = '4.7110'  # Replace with the desired latitude\n",
    "longitude = '74.072'  # Replace with the desired longitude\n",
    "\n",
    "# Make a request to the One Call API\n",
    "url = f'https://api.openweathermap.org/data/3.0/onecall?lat={latitude}&lon={longitude}&appid={api_key}'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the relevant information from the JSON response\n",
    "    current_temperature = data['current']['temp']\n",
    "    current_weather_description = data['current']['weather'][0]['description']\n",
    "\n",
    "    # Print the extracted information\n",
    "    print(f'Current Temperature: {current_temperature} K')\n",
    "    print(f'Current Weather: {current_weather_description}')\n",
    "else:\n",
    "    # Print an error message if the request was not successful\n",
    "    print(f'Error: {response.status_code}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

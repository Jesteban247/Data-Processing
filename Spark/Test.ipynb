{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease [3631 B]\n",
      "Get:2 https://dl.yarnpkg.com/debian stable InRelease [17.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]     \u001b[0m       \u001b[0m\u001b[33m\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \u001b[0m\n",
      "Get:5 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease [3960 B]0m\n",
      "Get:6 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main amd64 Packages [260 kB]\n",
      "Get:7 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main all Packages [2714 B]\n",
      "Get:8 https://dl.yarnpkg.com/debian stable/main all Packages [11.1 kB]         \u001b[0m\u001b[33m\n",
      "Get:9 https://dl.yarnpkg.com/debian stable/main amd64 Packages [11.1 kB]\n",
      "Get:11 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable/main amd64 Packages [3650 B]m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3196 kB]\n",
      "Get:10 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease [28.0 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3329 kB]m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]0m\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.7 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1170 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB][0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\u001b[33m\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1466 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3345 kB]33m\n",
      "Get:23 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 Packages [3505 B]m\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.4 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3806 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 30.2 MB in 3s (10.4 MB/s)[33m                      \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "Requirement already satisfied: pyspark in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: py4j in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/27 00:49:50 WARN Utils: Your hostname, codespaces-4f3c52 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/01/27 00:49:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/27 00:49:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://155e6883-2415-4565-bb48-26eafce4d7e5.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Our First Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ead341ae0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Example\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/27 00:50:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a DataFrame with some data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
